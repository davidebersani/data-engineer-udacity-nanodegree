{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Trending Youtube Video Statistics\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project uses a [dataset from Kaggle](https://www.kaggle.com/datasnaek/youtube-new) containing statistics for trending video on YouTube, collected daily. \n",
    "\n",
    "Please, download the dataset and extract it in this folder. The data should have the path \n",
    "```\n",
    "project_dir/\n",
    "    |- data\n",
    "    |   |- countries.json\n",
    "    |   |- CA_category_id.json\n",
    "    |   |- Cavideos.csv\n",
    "    ....\n",
    "    \n",
    "```\n",
    "\n",
    "The goal of this project is to structure and persist these data on a Data warehouse. For example, a Ads company can query data to discover which are the most trending artists, or to undestand which youtube channel/artist choose for an ads campaign.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import psycopg2\n",
    "import uuid\n",
    "import numpy\n",
    "\n",
    "\n",
    "from model.db import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I'm using data from this [dataset from Kaggle](https://www.kaggle.com/datasnaek/youtube-new). With these data I've the possibility to analyze statistics like views, likes, dislikes, comments by country, category, date and so on. I want to create a data warehouse were a business user can query data, create reports and take decisions based on data.\n",
    "\n",
    "#### About the dataset\n",
    "\n",
    "#### Context\n",
    "\n",
    "YouTube (the world-famous video sharing website) maintains a list of the [top trending videos](https://www.youtube.com/feed/trending) on the platform. [According to Variety magazine](http://variety.com/2017/digital/news/youtube-2017-top-trending-videos-music-videos-1202631416/), “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”), celebrity and/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.\n",
    "\n",
    "This dataset is a daily record of the top trending YouTube videos.\n",
    "\n",
    "Note that this dataset is a structurally improved version of [this dataset](https://www.kaggle.com/datasnaek/youtube).\n",
    "\n",
    "#### Content\n",
    "\n",
    "This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.\n",
    "\n",
    "EDIT: Now includes data from RU, MX, KR, JP and IN regions (Russia, Mexico, South Korea, Japan and India respectively) over the same time period.\n",
    "\n",
    "Each region’s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.\n",
    "\n",
    "The data also includes a `category_id` field, which varies between regions. To retrieve the categories for a specific video, find it in the associated `JSON`. One such file is included for each of the five regions in the dataset.\n",
    "\n",
    "For more information on specific columns in the dataset refer to the [column metadata](https://www.kaggle.com/datasnaek/youtube-new/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "An example of video data (csv files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df = pd.read_csv(\"data/CAvideos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "An example of json files, that describe the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df_cat = pd.read_json(\"data/CA_category_id.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cat[\"items\"].head().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Total rows\n",
    "len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking null values\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "# The dataset is very usable, no null values.\n",
    "# The only missing values are on the description. It can be possible, not all videos have a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[df[\"description\"].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# I decide to replace these NaN values with an empty string, because it's more appropriate. It's a string field, not a number.\n",
    "df.description = df.description.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tag = [none]\n",
    "# Substitute with empty string\n",
    "df.tags.replace({\"[none]\": \"\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking duplicates values\n",
    "df[df.duplicated()]\n",
    "old = len(df.index)\n",
    "df = df.drop_duplicates([\"video_id\", \"trending_date\"])\n",
    "print(\"Duplicated items found: \" + str(old - len(df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I'm creating a data warehouse, so I structured the data model as a start schema.\n",
    "![ER diagram data model](er_capstone_project.png)\n",
    "\n",
    "**Dimensions:**\n",
    "- Time: it's usually a standard dimension in a data warehouse. I'm using the time for two dimensions in the fact table: trending_date (the date when the video was trending on YouTube) and publication_date (the date when the video was published)\n",
    "- Category: it's a dimension that describe the video's category.\n",
    "- Country: the country where the video was trending in a specific date.  \n",
    "- Channel: it describes the channel that published the video. It has only one attribute (title) because on the dataset it's the only information available, but I decided to model it as a dimension because can be useful to analyze the data on this dimension and can be developed a pipeline to add other useful data about the channels, increasing the informations available for this dimension.\n",
    "\n",
    "The \"Video\" table is the fact table, where the facts are:\n",
    "- view: number of views\n",
    "- like: number of likes\n",
    "- dislike: number of dislike\n",
    "- comments_count: number of comments\n",
    "- comments_disabled: True if the comments are disabled on the video\n",
    "- ratings_disabled: True if the ratings are disabled for the video\n",
    "- video_error_or_removed: True if the video was removed or there was some errors during video processing\n",
    "\n",
    "Every video has also two attributes: title and descriptions.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Let's imagine that we have data like this every month. We need a data pipeline that process the new data and inject them in the data warehouse.\n",
    "\n",
    "The step to create and populate the data warehouse are:\n",
    "1. Create and populate the \"Country\" dimention. This step is necessary only the first time we populate the data warehouse, because it's indipendent of the new data. For populate this table, i'm going to use the \"countries.json\" file, that's a mapping between every country and it's two-digits code.\n",
    "2. Create and populate the \"Category\" dimension. In this step I'll read the json file describing the categories for each country and I'll insert the data into the category table.\n",
    "3. For every csv file, read the file, pre process it (preprocessing described at step 2 of this notebook) and get data to populate video, channel and time category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "In the notebook I test the data pipeline only for a subset of files. It should be industrialized if we want to use it in production and on every file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**4.1.1 Connection to Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Complete this dictionary with yuur cluster informations\n",
    "conn_data = {\n",
    "        \"host\": \"\",\n",
    "        \"dbname\": \"\",\n",
    "        \"user\": \"\",\n",
    "        \"password\": \"\",\n",
    "        \"port\": \"5439\"\n",
    "    }\n",
    "DATABASE_URI = 'redshift+psycopg2://{user}:{password}@{host}:{port}/{dbname}'.format(**conn_data)\n",
    "engine = create_engine(DATABASE_URI)\n",
    "Session = sessionmaker(bind=engine)\n",
    "s = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**4.1.2 Country dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries=[]\n",
    "with open('data/countries.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for item in data:\n",
    "        c = Country()\n",
    "        c.country_id = item[\"Code\"]\n",
    "        c.complete_name = item[\"Name\"]\n",
    "        countries.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,c in enumerate(countries):\n",
    "    if i%10==0:\n",
    "        print(f\"Insert n. {i}\")\n",
    "    s.add(c)\n",
    "    s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**4.1.3 Category dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test only on one json file\n",
    "file = \"data/CA_category_id.json\"\n",
    "categories=[]\n",
    "with open(file) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for item in data[\"items\"]:\n",
    "        cat = Category()\n",
    "        cat.category_id = item[\"id\"]\n",
    "        cat.etag = item[\"etag\"].replace('\"', '')\n",
    "        cat.title = item[\"snippet\"][\"title\"]\n",
    "        categories.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, cat in enumerate(categories):\n",
    "    print(f\"Inserting item n. {i}\")\n",
    "    s.add(cat)\n",
    "    s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Read csv and populate tables**\n",
    "\n",
    "Consider only one csv in the notebook implementaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename = \"data/CAvideos.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = len(df.index)\n",
    "print(f\"Items: {count}\")\n",
    "\n",
    "# I decide to replace these NaN values with an empty string, because it's more appropriate. It's a string field, not a number.\n",
    "df.description = df.description.fillna('')\n",
    "\n",
    "# Checking duplicates values\n",
    "df = df.drop_duplicates([\"video_id\", \"trending_date\"])\n",
    "print(\"Duplicated items found: \" + str(count - len(df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "channels = df.channel_title.unique().tolist()\n",
    "len(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only for tests\n",
    "# channels = channels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "channels_to_insert = [Channel(channel_id=str(uuid.uuid4()),title=c) for c in channels]\n",
    "for i,c in enumerate(channels_to_insert):\n",
    "    if i%10==0:\n",
    "        print(f\"Insert n. {i}\")\n",
    "    s.add(c)\n",
    "    s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "times = set()\n",
    "trending_date_format = \"%y.%d.%m\"\n",
    "df.trending_date = df.trending_date.apply(\n",
    "    lambda ts: datetime.strptime(ts, trending_date_format)\n",
    ")\n",
    "times.update(set(df.trending_date.unique().tolist()))\n",
    "pub_date_format = \"%Y-%m-%dT%H:%M:%S.000Z\"\n",
    "df.publish_time = df.publish_time.apply(\n",
    "    lambda ts: datetime.strptime(ts, pub_date_format)\n",
    ")\n",
    "times.update(set(df.publish_time.unique().tolist()))\n",
    "len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only test\n",
    "# times = list(times)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch = dt.datetime(1970, 1, 1)\n",
    "def unix_time_millis_str(t):\n",
    "    return str((t - epoch).total_seconds() * 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, time in enumerate(times):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Insert n. {i}\")\n",
    "    time = dt.datetime.fromtimestamp(time/1000000000)\n",
    "    t = Time()\n",
    "    t.time_id = unix_time_millis_str(time)\n",
    "    t.day = time.day\n",
    "    t.month = time.month\n",
    "    t.year = time.year\n",
    "    t.hour = time.hour\n",
    "    t.minute = time.minute\n",
    "    s.add(t)\n",
    "    s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country = filename.split(\"/\")[-1].split(\"videos\")[0]\n",
    "country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_fact_row(row):\n",
    "    video = Video()\n",
    "    video.video_id = row[\"video_id\"]\n",
    "    video.title = row[\"title\"]\n",
    "    video.description = row[\"short_description\"]\n",
    "    video.view = int(row[\"views\"])\n",
    "    video.like = int(row[\"likes\"])\n",
    "    video.dislike = int(row[\"dislikes\"])\n",
    "    video.comments_count = int(row[\"comment_count\"])\n",
    "    video.comments_disabled = bool(row[\"comments_disabled\"])\n",
    "    video.ratings_disabled = bool(row[\"ratings_disabled\"])\n",
    "    video.video_error_or_removed = bool(row[\"video_error_or_removed\"])\n",
    "    video.category_id = int(row[\"category_id\"])\n",
    "    video.trending_date = unix_time_millis_str(row[\"trending_date\"])\n",
    "    video.publication_date = unix_time_millis_str(row[\"publish_time\"])\n",
    "    video.country_id = country\n",
    "    res = s.query(Channel).filter_by(title=row[\"channel_title\"])\n",
    "    try:\n",
    "        video.channel_id = res[0].channel_id\n",
    "    except Exception:\n",
    "        video.channel_id = None\n",
    "        \n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['short_description'] = df['description'].str[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test for one item\n",
    "# v = build_fact_row(df.iloc[0])\n",
    "# v.channel_id\n",
    "# s.add(v)\n",
    "# s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df = df.head(10) # Only for tests\n",
    "df[\"db_items\"] = df.apply(lambda row: build_fact_row(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "videos = df.db_items.tolist()\n",
    "len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,v in enumerate(videos):\n",
    "    if i%10==0:\n",
    "        print(f\"Insert n. {i}\")\n",
    "    \n",
    "    s.add(v)\n",
    "    s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "assert s.query(Country).count() == len(countries)\n",
    "assert s.query(Category).count() == len(categories)\n",
    "assert s.query(Channel).count() > 0\n",
    "assert s.query(Video).count() > 0\n",
    "\n",
    "res = s.query(Video, Country).join(Country).filter(Video.country_id is not None).limit(1)\n",
    "for r in res[0]:\n",
    "    assert r is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data dictionary is the Excel file called \"capstone_project_data_catalog.xlsx\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### Step 5: Complete Project Write Up\n",
    "##### Tools and technologies\n",
    "For this project, I chose to use Redshift to store data. The main goal of the project is to create a data warehouse, where business users can query data to have statistics about YouTube trending videos. Redshift is the perfect solution in this case, because it allows a big flexibility and efficiency, can scale on a huge data size and allows business users to query data with SQL.\n",
    "\n",
    "For data processing, I chose to use Pandas. It's a data analysis library that allows to explore and manipulate data. In the notebook I processed only one file at a time and Pandas can handle its dimension.   \n",
    "\n",
    "I also used SQLAlchemy as ORM for Python. In fact, once decided the star schema, I wrote the model classes in a python file. In this way, manipulating database object, queries and transactions is easier.\n",
    " \n",
    "##### Pipeline scheduling\n",
    "This dataset is a collection of data from November 2017 to June 2018, but the data have a daily granularity. So, the data can be updated every day or, if the analyses are not so urgent, the data can be updated every week. A cronjob can generate the dataset (using the process described here [https://github.com/mitchelljy/Trending-YouTube-Scraper](https://github.com/mitchelljy/Trending-YouTube-Scraper))  and the ETL pipeline can populate the data warehouse.\n",
    "\n",
    "##### Different scenario: data increased by 100x\n",
    "In this case, tools like Pandas are not an option anymore. To pre-process data, I would use a Spark cluster and I would convert the pre-processing step into a Spark job. To orchestrate the jobs, I would use an Airflow data pipeline. SQLAlchemy is always a good tool to use. For data storage, I would continue using Redshift, because it can scale horizontally and handle the data increase. \n",
    "\n",
    "##### Different scenario: The data populates a dashboard that must be updated on a daily basis by 7am every day\n",
    "In this case, the data pipeline should be scheduled for updating the data warehouse every day. The pipeline could be scheduled, for example, at 1 a.m. to retrieve data of the past day. After the data warehouse is updated, an Airflow pipeline scheduled, for example, at 3 a.m. can update the dashboard.\n",
    "\n",
    "##### Different scenario: The database needed to be accessed by 100+ people\n",
    "For supporting more people accessing the data warehouse, I thought to some possible improvements:\n",
    "- Generate OLAP Cubes to improve common queries\n",
    "- Ask to user if there are common queries or  standard queries that they will execute every day at 100%, so we can execute them only once and generate report for all interested users.\n",
    "- Scale redhsift horizontally to support more queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
